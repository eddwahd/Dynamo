======================
TODO list for Dynamo 2
======================

(*) = done


1. Sample/reference problem set, unit tests
     (1) Include all the reference problems from Dynamo 1.2
      2. Additional demos (covering all the new features/tasks)
         *  open systems, _partial tasks
         *  optimizing taus
      3. Tests
        (*) test_gradient
         *  script that runs all the demos (to make sure they still run!)
         *  the same unitary (closed) optimization problem should yield a similar solution in both "closed" and "open" modes
         *  the same unitary sequence must return the same error in both "closed" and "open" modes

2. Minimal benchmarking framework
      1. Allow user to easily compare several algorithms for a given problem
      2. Produce spaghetti plots

3. Robust data provenance, reproducibility
     (1) Store in the dataset all necessary metadata, such as
         Dynamo version, a timestamp, author name, description string etc.
           * Going from description to H is easy, H to description hard/impossible.
      2. Store the optimization parameters, initial sequence etc. so
         the exact same optimization can be re-run any time. Add a
         function for this.
     (3) In the system description, include also dimension vector and
         labels for the states and controls.

4. Controls: transformations, limits, nonlinearity etc.
     (1) Implementation method: "fold" controls through a linearly scaled "sin" function
     (2) taus as optimization parameters
      3. Nondiagonal transformations, i.e. x = A cos(B), y = A sin(B). 
      4. Shai's Jacobian stack?
           * Strange bases for controls: CRAB?
      5. Preconditioning?

5. Additional documentation
      1. Inline in the code
           (*) Modify some variable names to be self-documenting
      2. At function headers so Matlab's "help" gives more meaningful results
      3. Wiki
      4. Additional examples, see also 1.2
      5. Add reference to github for those wishing to contribute
      6. dynamo_manual.tex

6. More optimization tasks
      1. Closed S+E, where we only care about S
           * (unitary gates), state transfer
      2. Open S+E, Markovian evolution, where we only care about S.
           * unitary gates/general maps, (state transfer)

7. More algorithms
      1. "Integrators"
           * (expm), (eigendecomposition), expv, ode solvers (for time-dependent H:s and H_c:s), t-DMRG?
      2. Gradients
           * (GRAPE), (exact, finite diff, 1st order)
      3. Optimizers
           * (k-BFGS, using fminunc), Newton-Raphson, Monotonic Krotov variant?, Simplex? (no gradient needed)

8. User interface / features
      1. More button-type ui controls for the ui window
          *  Enable/disable tau optimization
          *  save a copy/snapshot
          *  pause
          *  plotting interval
          *  bin splitting
      2. Better API
         (*) Replace s and sb with closed, open
          *  fix other bad/undescriptive names 
      3. Other features
          *  fix the stats system so that an interrupted optimization run can be continued
          *  automatic top curve sweeping function


Random ideas and things to test:

- target = id with relaxation

- S+E, S+E+B: Kosut's paper on non-markovian distance measures:
  http://arxiv.org/pdf/quant-ph/0606064v1

- When are the controls kind of self-limited? Only if we need to wait
  for H_drift (coupling) to complete the operation?

- UI: monitor_func is not called reliably, thus optimization does not always
  terminate when trying to close the UI window

- express a state transfer problem in a Hermitian basis => real
  vectors and matrices instead of complex ones, faster...

- Ask Frank about noncommuting dP/du algorithms

- consistent function interface: dim as first or second parameter?

- Module for nth-degree gradient approximation with repeated scaling
  and squaring: Fouquieres et al., J. Magn. Reson. 212, 412 (2011).
  Automatic choice of \epsilon for finite-difference gradient method, ibid.

- Augmented matrix exponential gradient module for open systems, Floether et al. (2012)

- performance-compare error_full and the approximation given by error_real

- compare top-curve-sweeping for determining optimal T with direct tau-optimizing

- Lots of profiling. why do we use almost 30% more time in task1 than Dynamo 1.2? recompute_timeslots is called more?!?
  we seem to need more optimization rounds, maybe sth wrong with the gradient?

- error_abs and error_abs2 seem to give similar performance (as they should) (roughly same N_eval).

- maybe it's not a good idea to normalize the error function with
  |X_f|^2 when doing state transfer since it (being the purity) varies?

- check that the chosen gradient func matches with the error func!

- fix the 'none yet' term reason when fminunc decides to terminate on its own

- make setting control bounds/params easier/more automated

- make sure that (printed) error function values are (1) somehow physical, (2) same/comparable between all optimization tasks

- problems resulting from using an approximated gradient (incl. finite diff with bad epsilon): breaking symmetries in controls, bad convergence, fake "dynamic" local minima?

- decide what to do with the 1st order gradients, they're awful. see @dynamo/gradient_test.m
